


## Import and Initialization
```
import numpy as np
import torch
# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype='float32')
# Targets (apples, oranges)
targets = np.array([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype='float32')
# Convert inputs and targets to tensors
inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)
w = torch.randn(2, 3, requires_grad=True)
b = torch.randn(2, requires_grad=True)

```

![[Pasted image 20231203234729.png]]
* Columns is decided by count of outputs

## Functions 
```
# Model function
def model(x):
    return x @ w.t() + b
# MSE loss
def mse(t1, t2):
    diff = t1 - t2
    return torch.sum(diff * diff) / diff.numel()
```
**Note** : function "numel()"

## Train the model using gradient descent

As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can _train_ the model using the following steps:
1. Generate predictions
2. Calculate the loss
3. Compute gradients w.r.t the weights and biases
4. Adjust the weights by subtracting a small quantity proportional to the gradient
5. Reset the gradients to zero
Let's implement the above step by step.

```
# Train for 100 epochs
for i in range(100):
	# Generate Prediction
    preds = model(inputs)
    # Calculate Loss
    loss = mse(preds, targets)
    # Compute Gradients
    loss.backward() # w.grad, b.grad
    # Adjust weights & reset gradients
    with torch.no_grad():
        w -= w.grad * 1e-5
        b -= b.grad * 1e-5
        w.grad.zero_()
        b.grad.zero_()
```